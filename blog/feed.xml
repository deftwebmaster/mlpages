<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>Root Cause</title>
    <link href="https://mattlivingston.com/blog/feed.xml" rel="self" />
    <link href="https://mattlivingston.com/blog" />
    <updated>2026-01-01T17:25:27-06:00</updated>
    <author>
        <name>Matt Livingston</name>
    </author>
    <id>https://mattlivingston.com/blog</id>

    <entry>
        <title>What Warehouse Ops and Web Dev Have in Common</title>
        <author>
            <name>Matt Livingston</name>
        </author>
        <link href="https://mattlivingston.com/blog/what-warehouse-ops-and-web-dev-have-in-common/"/>
        <id>https://mattlivingston.com/blog/what-warehouse-ops-and-web-dev-have-in-common/</id>
        <media:content url="https://mattlivingston.com/blog/media/posts/5/ops-web-dev.png" medium="image" />

        <updated>2026-01-01T17:24:27-06:00</updated>
            <summary type="html">
                <![CDATA[
                        <img src="https://mattlivingston.com/blog/media/posts/5/ops-web-dev.png" alt="" />
                    Warehouse operations and web development look like completely different disciplines. One involves forklifts, scanners, and physical inventory. The other involves code, APIs, and servers. But strip away the tools and the work is structurally identical: maintaining data integrity across imperfect systems operated by humans. The&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://mattlivingston.com/blog/media/posts/5/ops-web-dev.png" class="type:primaryImage" alt="" /></p>
                <p>Warehouse operations and web development look like completely different disciplines. One involves forklifts, scanners, and physical inventory. The other involves code, APIs, and servers. But strip away the tools and the work is structurally identical: maintaining data integrity across imperfect systems operated by humans.</p><p>The core failures are the same. The disciplines required to prevent them are the same. The only difference is latency—how quickly the system punishes you for being wrong.</p><p>I’ve spent five years in warehouse operations using Oracle Fusion and Manhattan WMS, reconciling inventory discrepancies and tracing system failures. I’ve also built web tools for warehouse workflows—barcode validators, cycle count reconcilers, load calculators. The thinking required for both is identical. The assumptions people make about which one is “analytical” are wrong.</p><p>Warehouse work isn’t operational and reactive. Web dev isn’t uniquely analytical and creative. Both punish sloppy thinking. Both reward precision, skepticism, and discipline. Here’s how.</p><h2 id="data-integrity-systems-lying-politely">Data Integrity: Systems Lying Politely</h2>
<p><strong>Warehouse scenario:</strong><br>Inventory quantities diverge because cycle counts, adjustments, and replenishments aren’t reconciled against a single source of truth. You check one screen: 150 units on hand. You check another: 138. Physical count says 142. All three numbers exist in the system simultaneously, each with timestamps, each claiming to be correct.</p><p>Which one is real? The answer depends on which process last touched the data and whether that process successfully propagated changes to dependent tables. The system doesn’t enforce consistency—it just logs transactions and hopes downstream processes reconcile eventually.</p><p><strong>Web dev scenario:</strong><br>I built a browser tool where users could save configuration settings. The frontend showed “saved” after form submission. The backend insert sometimes failed silently due to constraint violations, but returned a 200 status anyway. The UI said the data persisted. The database disagreed. Users trusted the interface. Their settings vanished.</p><p>Two sources of truth existed. The system lied politely—success signals without state verification.</p><p><strong>The parallel:</strong><br>In both cases, the failure isn’t a bug in a single component. It’s a contract violation between parts of a system. One part says “this happened.” Another part never confirms it. The system proceeds as if consensus exists when it doesn’t.</p><p>The discipline required: <strong>never trust success signals without verifying state change.</strong> Confirmation is not the same as correctness.</p><h2 id="system-failures-retries-without-safeguards">System Failures: Retries Without Safeguards</h2>
<p><strong>Warehouse scenario:</strong><br>During high-volume operations, scanner confirmations sometimes lag. An operator scans a putaway, sees no feedback, scans again. The system processes both as separate events. Inventory that moved once gets counted twice. Downstream processes—replenishment, picking, shipping—all execute based on inflated numbers. The lie propagates cleanly because every step trusts the data it receives.</p><p><strong>Web dev scenario:</strong><br>A form submission on a slow network connection times out from the user’s perspective. They click “submit” again. The server processes both requests. No idempotency check, no deduplication logic. Two records get created. If the form was an order, the customer gets charged twice. If it was a reservation, inventory gets double-booked.</p><p><strong>The parallel:</strong><br>Retries are a natural human response to unresponsive systems. If your system doesn’t account for retries—if it assumes every event is unique and intentional—it will create duplicates, phantom states, and cascading failures.</p><p>The discipline required: <strong>design for retries.</strong> Idempotency isn’t optional. Every action that changes state must be safe to repeat.</p><h2 id="root-cause-analysis-symptoms-live-downstream-causes-live-upstream">Root Cause Analysis: Symptoms Live Downstream, Causes Live Upstream</h2>
<p><strong>Warehouse scenario:</strong><br>A pick task fails. The system says inventory exists in a location. The picker goes there. Nothing. The obvious assumption: the picker is wrong, or someone stole product, or the count is off.</p><p>Root cause analysis traces backward: Why did the system think inventory was there? Because replenishment moved it. Why did replenishment move it? Because slotting logic said the location was optimal. Why did slotting say that? Because forecasted demand triggered a threshold. Why was demand forecasted that way? Because upstream receipts were logged incorrectly three weeks ago.</p><p>The failure appears at the pick. The cause is buried in receiving.</p><p><strong>Web dev scenario:</strong><br>A feature breaks. A button doesn’t respond. The obvious assumption: the button handler is broken.</p><p>Root cause analysis traces backward: Why isn’t the handler firing? Because the event listener didn’t attach. Why didn’t it attach? Because the DOM element didn’t exist when the script ran. Why didn’t it exist? Because a race condition in data fetching delayed rendering. Why was there a race? Because a cache invalidation assumption was wrong two layers upstream.</p><p>The failure appears at the button. The cause is in state initialization.</p><p><strong>The parallel:</strong><br>Symptoms appear where users interact with the system. Causes live where the system makes assumptions about state, timing, or dependencies. Debugging is tracing backward through handoffs until you find the first lie.</p><p>The discipline required: <strong>distrust the obvious.</strong> The nearest broken thing is rarely the root cause.</p><h2 id="documentation-undocumented-systems-rot">Documentation: Undocumented Systems Rot</h2>
<p><strong>Warehouse scenario:</strong><br>Experienced operators know not to use a particular WMS screen after 3pm because it conflicts with nightly batch processing. They know certain SKUs “act weird” during cycle counts because of unit-of-measure mismatches in the item master. They know which receiving doors cause barcode scanning issues because of poor lighting.</p><p>None of this is documented. When those operators leave, or when new staff arrive, the failures repeat. Every time, someone rediscovers the problem through pain. The system technically works, but only if you know the invisible rules.</p><p><strong>Web dev scenario:</strong><br>A tool I built had an edge case: if a user uploaded a file larger than 5MB, the browser would hang for several seconds before timing out. I knew this. I worked around it by keeping files small during testing. I didn’t document it.</p><p>Six months later, I forgot. A user hit the edge case. The tool felt broken. I debugged it again, rediscovered the limitation, and added validation. The system technically worked before—but only if you remembered the constraint.</p><p><strong>The parallel:</strong><br>Undocumented constraints, workarounds, and edge cases create fragile systems that only function under unstated conditions. The knowledge exists in someone’s head, not in the system. When that knowledge is lost, the system degrades even though the code or process hasn’t changed.</p><p>The discipline required: <strong>document constraints, not just features.</strong> If it breaks under certain conditions, write it down. If it works only because of an assumption, state the assumption.</p><h2 id="the-structural-overlap">The Structural Overlap</h2>
<p>Once you strip away the tools and titles, both jobs reward the same habits:</p><ul>
<li><strong>Respect for data:</strong> Trust nothing. Verify state. Reconcile sources of truth.</li>
<li><strong>Suspicion of success signals:</strong> A closed transaction is not the same as a correct outcome.</li>
<li><strong>Patience with causality:</strong> Effects appear downstream. Causes live upstream. Trace backward.</li>
<li><strong>Discipline in documentation:</strong> Undocumented systems rot. Write down what breaks and why.</li>
</ul>
<p>The overlap isn’t a coincidence. It’s structural. Both fields deal with distributed state, imperfect inputs, human behavior, and the gap between what a system claims and what actually happened.</p><p>Warehouse systems just make the failure visible faster. In web dev, you can be wrong for weeks before anyone notices. In a warehouse, you’re wrong the moment the picker opens an empty bin.</p><p>That’s the only real difference: how quickly reality forces reckoning.</p><h2 id="why-this-matters">Why This Matters</h2>
<p>The assumption that warehouse operations is reactive and web development is analytical is a misclassification of skill. Both require the same foundational thinking. Both punish sloppiness the same way. The medium changes. The discipline doesn’t.</p><p>I’m not “transitioning” into web development. I’m applying the same thinking in a different medium. The tools are different. The problems are identical.</p><p>If you’ve debugged inventory discrepancies, you’ve already debugged distributed systems. If you’ve traced a warehouse failure through logs, timestamps, and process boundaries, you’ve already done root cause analysis in production.</p><p>The skills transfer because the problems are the same. The interface is the only thing that changes.</p>
            ]]>
        </content>
    </entry>
</feed>
